## Entropy
Measure of uncertainty, or the expected amount of information in an even drawn from from distribution;

 H(x)=E[I(x)]=  -&Sigma; (p(x) log p(x)), 
 
 where I(x) is self information I(x)= -log p(x)

- low entropy --> distr. with certain outcome
- high entropy --> uniform distribution

### Experiment
We try what the NN learns if we minimize entropy of the topic distribution, in unsupervised fashion.
Since we do not give any criteria on how to assign or cluster the topics, we expect the model to learn one topic
assignment for all actions.

As expected, the policy does not perform well and simply learns to assign all actions to the same topic. 
